{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps\n",
    "\n",
    "The data being used in this analysis are imbalanced, therefore we will have to think about how to best determine the accuracy of our models. The first instict would be to use auPRC to determine accuracy of our models. Secondly, we will need to think about some normalization techniques bbefore training of our models. \n",
    "\n",
    "1. Extract all the test data\n",
    "2. Automate the whole analsysis form start to finish\n",
    "3. Optimize algorithm by choosing the relevant data\n",
    "4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clinical_df = pd.read_csv('../Data/Clinical_Data/sc1_Training_ClinAnnotations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data = clinical_df[['Patient', 'D_Age', 'D_Gender', 'D_ISS','HR_FLAG']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data = training_data.set_index('Patient')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>D_Age</th>\n",
       "      <th>D_Gender</th>\n",
       "      <th>D_ISS</th>\n",
       "      <th>HR_FLAG</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Patient</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MMRF_1016</th>\n",
       "      <td>56</td>\n",
       "      <td>Male</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FALSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1021</th>\n",
       "      <td>54</td>\n",
       "      <td>Female</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FALSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1029</th>\n",
       "      <td>46</td>\n",
       "      <td>Male</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FALSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1030</th>\n",
       "      <td>65</td>\n",
       "      <td>Female</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FALSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1031</th>\n",
       "      <td>62</td>\n",
       "      <td>Male</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FALSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1032</th>\n",
       "      <td>76</td>\n",
       "      <td>Male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>FALSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1033</th>\n",
       "      <td>64</td>\n",
       "      <td>Male</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1037</th>\n",
       "      <td>81</td>\n",
       "      <td>Female</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CENSORED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1038</th>\n",
       "      <td>69</td>\n",
       "      <td>Male</td>\n",
       "      <td>3.0</td>\n",
       "      <td>FALSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1045</th>\n",
       "      <td>81</td>\n",
       "      <td>Female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FALSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1048</th>\n",
       "      <td>53</td>\n",
       "      <td>Female</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FALSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1049</th>\n",
       "      <td>68</td>\n",
       "      <td>Female</td>\n",
       "      <td>2.0</td>\n",
       "      <td>FALSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1054</th>\n",
       "      <td>58</td>\n",
       "      <td>Female</td>\n",
       "      <td>2.0</td>\n",
       "      <td>FALSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1055</th>\n",
       "      <td>66</td>\n",
       "      <td>Male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1061</th>\n",
       "      <td>70</td>\n",
       "      <td>Female</td>\n",
       "      <td>2.0</td>\n",
       "      <td>FALSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1068</th>\n",
       "      <td>58</td>\n",
       "      <td>Female</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1073</th>\n",
       "      <td>62</td>\n",
       "      <td>Female</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FALSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1077</th>\n",
       "      <td>67</td>\n",
       "      <td>Male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>FALSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1078</th>\n",
       "      <td>45</td>\n",
       "      <td>Male</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FALSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1080</th>\n",
       "      <td>66</td>\n",
       "      <td>Male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>FALSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1082</th>\n",
       "      <td>47</td>\n",
       "      <td>Male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1086</th>\n",
       "      <td>66</td>\n",
       "      <td>Female</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FALSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1089</th>\n",
       "      <td>61</td>\n",
       "      <td>Male</td>\n",
       "      <td>3.0</td>\n",
       "      <td>FALSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1090</th>\n",
       "      <td>68</td>\n",
       "      <td>Female</td>\n",
       "      <td>3.0</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1092</th>\n",
       "      <td>69</td>\n",
       "      <td>Male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>FALSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1098</th>\n",
       "      <td>49</td>\n",
       "      <td>Male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>FALSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1100</th>\n",
       "      <td>68</td>\n",
       "      <td>Female</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FALSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1107</th>\n",
       "      <td>82</td>\n",
       "      <td>Male</td>\n",
       "      <td>3.0</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1108</th>\n",
       "      <td>85</td>\n",
       "      <td>Female</td>\n",
       "      <td>3.0</td>\n",
       "      <td>FALSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1128</th>\n",
       "      <td>48</td>\n",
       "      <td>Female</td>\n",
       "      <td>2.0</td>\n",
       "      <td>FALSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2563</th>\n",
       "      <td>62</td>\n",
       "      <td>Male</td>\n",
       "      <td>3.0</td>\n",
       "      <td>CENSORED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2564</th>\n",
       "      <td>55</td>\n",
       "      <td>Female</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CENSORED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2566</th>\n",
       "      <td>57</td>\n",
       "      <td>Male</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CENSORED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2567</th>\n",
       "      <td>62</td>\n",
       "      <td>Male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>CENSORED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2570</th>\n",
       "      <td>61</td>\n",
       "      <td>Female</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CENSORED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2572</th>\n",
       "      <td>75</td>\n",
       "      <td>Male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>CENSORED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2573</th>\n",
       "      <td>72</td>\n",
       "      <td>Male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>CENSORED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2574</th>\n",
       "      <td>74</td>\n",
       "      <td>Male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>CENSORED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2577</th>\n",
       "      <td>51</td>\n",
       "      <td>Male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>CENSORED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2579</th>\n",
       "      <td>61</td>\n",
       "      <td>Male</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CENSORED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2580</th>\n",
       "      <td>80</td>\n",
       "      <td>Female</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CENSORED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2581</th>\n",
       "      <td>72</td>\n",
       "      <td>Male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>CENSORED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2587</th>\n",
       "      <td>72</td>\n",
       "      <td>Female</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CENSORED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2590</th>\n",
       "      <td>72</td>\n",
       "      <td>Female</td>\n",
       "      <td>3.0</td>\n",
       "      <td>CENSORED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2592</th>\n",
       "      <td>59</td>\n",
       "      <td>Female</td>\n",
       "      <td>3.0</td>\n",
       "      <td>CENSORED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2593</th>\n",
       "      <td>65</td>\n",
       "      <td>Male</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CENSORED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2595</th>\n",
       "      <td>60</td>\n",
       "      <td>Female</td>\n",
       "      <td>2.0</td>\n",
       "      <td>CENSORED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2596</th>\n",
       "      <td>73</td>\n",
       "      <td>Female</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2597</th>\n",
       "      <td>64</td>\n",
       "      <td>Female</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CENSORED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2598</th>\n",
       "      <td>82</td>\n",
       "      <td>Female</td>\n",
       "      <td>3.0</td>\n",
       "      <td>CENSORED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2599</th>\n",
       "      <td>71</td>\n",
       "      <td>Female</td>\n",
       "      <td>2.0</td>\n",
       "      <td>CENSORED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2601</th>\n",
       "      <td>70</td>\n",
       "      <td>Female</td>\n",
       "      <td>2.0</td>\n",
       "      <td>CENSORED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2602</th>\n",
       "      <td>51</td>\n",
       "      <td>Male</td>\n",
       "      <td>3.0</td>\n",
       "      <td>CENSORED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2605</th>\n",
       "      <td>62</td>\n",
       "      <td>Female</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CENSORED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2606</th>\n",
       "      <td>70</td>\n",
       "      <td>Female</td>\n",
       "      <td>2.0</td>\n",
       "      <td>CENSORED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2608</th>\n",
       "      <td>67</td>\n",
       "      <td>Male</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CENSORED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2611</th>\n",
       "      <td>66</td>\n",
       "      <td>Female</td>\n",
       "      <td>2.0</td>\n",
       "      <td>CENSORED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2613</th>\n",
       "      <td>70</td>\n",
       "      <td>Female</td>\n",
       "      <td>2.0</td>\n",
       "      <td>CENSORED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2614</th>\n",
       "      <td>58</td>\n",
       "      <td>Male</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CENSORED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2622</th>\n",
       "      <td>59</td>\n",
       "      <td>Male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>CENSORED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>737 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           D_Age D_Gender  D_ISS   HR_FLAG\n",
       "Patient                                   \n",
       "MMRF_1016     56     Male    1.0     FALSE\n",
       "MMRF_1021     54   Female    1.0     FALSE\n",
       "MMRF_1029     46     Male    1.0     FALSE\n",
       "MMRF_1030     65   Female    1.0     FALSE\n",
       "MMRF_1031     62     Male    1.0     FALSE\n",
       "MMRF_1032     76     Male    2.0     FALSE\n",
       "MMRF_1033     64     Male    1.0      TRUE\n",
       "MMRF_1037     81   Female    1.0  CENSORED\n",
       "MMRF_1038     69     Male    3.0     FALSE\n",
       "MMRF_1045     81   Female    NaN     FALSE\n",
       "MMRF_1048     53   Female    1.0     FALSE\n",
       "MMRF_1049     68   Female    2.0     FALSE\n",
       "MMRF_1054     58   Female    2.0     FALSE\n",
       "MMRF_1055     66     Male    2.0      TRUE\n",
       "MMRF_1061     70   Female    2.0     FALSE\n",
       "MMRF_1068     58   Female    1.0      TRUE\n",
       "MMRF_1073     62   Female    1.0     FALSE\n",
       "MMRF_1077     67     Male    2.0     FALSE\n",
       "MMRF_1078     45     Male    1.0     FALSE\n",
       "MMRF_1080     66     Male    2.0     FALSE\n",
       "MMRF_1082     47     Male    2.0      TRUE\n",
       "MMRF_1086     66   Female    1.0     FALSE\n",
       "MMRF_1089     61     Male    3.0     FALSE\n",
       "MMRF_1090     68   Female    3.0      TRUE\n",
       "MMRF_1092     69     Male    2.0     FALSE\n",
       "MMRF_1098     49     Male    2.0     FALSE\n",
       "MMRF_1100     68   Female    1.0     FALSE\n",
       "MMRF_1107     82     Male    3.0      TRUE\n",
       "MMRF_1108     85   Female    3.0     FALSE\n",
       "MMRF_1128     48   Female    2.0     FALSE\n",
       "...          ...      ...    ...       ...\n",
       "MMRF_2563     62     Male    3.0  CENSORED\n",
       "MMRF_2564     55   Female    1.0  CENSORED\n",
       "MMRF_2566     57     Male    1.0  CENSORED\n",
       "MMRF_2567     62     Male    2.0  CENSORED\n",
       "MMRF_2570     61   Female    1.0  CENSORED\n",
       "MMRF_2572     75     Male    2.0  CENSORED\n",
       "MMRF_2573     72     Male    2.0  CENSORED\n",
       "MMRF_2574     74     Male    2.0  CENSORED\n",
       "MMRF_2577     51     Male    2.0  CENSORED\n",
       "MMRF_2579     61     Male    1.0  CENSORED\n",
       "MMRF_2580     80   Female    1.0  CENSORED\n",
       "MMRF_2581     72     Male    2.0  CENSORED\n",
       "MMRF_2587     72   Female    1.0  CENSORED\n",
       "MMRF_2590     72   Female    3.0  CENSORED\n",
       "MMRF_2592     59   Female    3.0  CENSORED\n",
       "MMRF_2593     65     Male    1.0  CENSORED\n",
       "MMRF_2595     60   Female    2.0  CENSORED\n",
       "MMRF_2596     73   Female    1.0      TRUE\n",
       "MMRF_2597     64   Female    1.0  CENSORED\n",
       "MMRF_2598     82   Female    3.0  CENSORED\n",
       "MMRF_2599     71   Female    2.0  CENSORED\n",
       "MMRF_2601     70   Female    2.0  CENSORED\n",
       "MMRF_2602     51     Male    3.0  CENSORED\n",
       "MMRF_2605     62   Female    1.0  CENSORED\n",
       "MMRF_2606     70   Female    2.0  CENSORED\n",
       "MMRF_2608     67     Male    1.0  CENSORED\n",
       "MMRF_2611     66   Female    2.0  CENSORED\n",
       "MMRF_2613     70   Female    2.0  CENSORED\n",
       "MMRF_2614     58     Male    1.0  CENSORED\n",
       "MMRF_2622     59     Male    2.0  CENSORED\n",
       "\n",
       "[737 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#(training_data.HR_FLAG == 'FALSE') or (training_data.HR_FLAG == 'TRUE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A simple function for coding gender\n",
    "def code_gender(f):\n",
    "    g_list = []\n",
    "    for f in female:\n",
    "        if f == 'Female':\n",
    "            g_list.append(0)\n",
    "        else:\n",
    "            g_list.append(1)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#real_data['D_PFS_FLAG_18'] = pd.get_dummies((real_data['D_PFS']/30.4167 >18) & (real_data['D_PFS_FLAG'] == 1) )[True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Encode gender and labels\n",
    "\n",
    "The issue here is how to deal with the CENSORED data? Do we need to make predictions on these?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data['Gender'] = pd.get_dummies(training_data['D_Gender'])['Male']\n",
    "training_data.drop('D_Gender', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data = training_data.loc[training_data['HR_FLAG'].isin(['FALSE','TRUE'])]\n",
    "\n",
    "training_data['FLAG'] = pd.get_dummies(training_data['HR_FLAG'])['TRUE']\n",
    "training_data.drop('HR_FLAG', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_df = real_data['D_Age\tD_OS_FLAG\tD_PFS\tD_PFS_FLAG\tD_ISS\tD_PFS_FLAG_18\tGender'.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>D_Age</th>\n",
       "      <th>D_ISS</th>\n",
       "      <th>Gender</th>\n",
       "      <th>FLAG</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Patient</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MMRF_1016</th>\n",
       "      <td>56</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1021</th>\n",
       "      <td>54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1029</th>\n",
       "      <td>46</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1030</th>\n",
       "      <td>65</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1031</th>\n",
       "      <td>62</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1032</th>\n",
       "      <td>76</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1033</th>\n",
       "      <td>64</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1038</th>\n",
       "      <td>69</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1045</th>\n",
       "      <td>81</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1048</th>\n",
       "      <td>53</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1049</th>\n",
       "      <td>68</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1054</th>\n",
       "      <td>58</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1055</th>\n",
       "      <td>66</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1061</th>\n",
       "      <td>70</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1068</th>\n",
       "      <td>58</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1073</th>\n",
       "      <td>62</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1077</th>\n",
       "      <td>67</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1078</th>\n",
       "      <td>45</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1080</th>\n",
       "      <td>66</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1082</th>\n",
       "      <td>47</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1086</th>\n",
       "      <td>66</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1089</th>\n",
       "      <td>61</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1090</th>\n",
       "      <td>68</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1092</th>\n",
       "      <td>69</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1098</th>\n",
       "      <td>49</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1100</th>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1107</th>\n",
       "      <td>82</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1108</th>\n",
       "      <td>85</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1128</th>\n",
       "      <td>48</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_1129</th>\n",
       "      <td>81</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2266</th>\n",
       "      <td>73</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2272</th>\n",
       "      <td>57</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2273</th>\n",
       "      <td>69</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2289</th>\n",
       "      <td>52</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2290</th>\n",
       "      <td>79</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2300</th>\n",
       "      <td>80</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2306</th>\n",
       "      <td>40</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2308</th>\n",
       "      <td>74</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2314</th>\n",
       "      <td>59</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2335</th>\n",
       "      <td>64</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2344</th>\n",
       "      <td>66</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2364</th>\n",
       "      <td>51</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2373</th>\n",
       "      <td>67</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2379</th>\n",
       "      <td>79</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2386</th>\n",
       "      <td>64</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2404</th>\n",
       "      <td>50</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2412</th>\n",
       "      <td>65</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2444</th>\n",
       "      <td>81</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2455</th>\n",
       "      <td>63</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2457</th>\n",
       "      <td>52</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2473</th>\n",
       "      <td>74</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2477</th>\n",
       "      <td>61</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2478</th>\n",
       "      <td>52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2480</th>\n",
       "      <td>63</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2490</th>\n",
       "      <td>81</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2535</th>\n",
       "      <td>46</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2554</th>\n",
       "      <td>65</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2557</th>\n",
       "      <td>83</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2562</th>\n",
       "      <td>66</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF_2596</th>\n",
       "      <td>73</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>482 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           D_Age  D_ISS  Gender  FLAG\n",
       "Patient                              \n",
       "MMRF_1016     56    1.0     1.0   0.0\n",
       "MMRF_1021     54    1.0     0.0   0.0\n",
       "MMRF_1029     46    1.0     1.0   0.0\n",
       "MMRF_1030     65    1.0     0.0   0.0\n",
       "MMRF_1031     62    1.0     1.0   0.0\n",
       "MMRF_1032     76    2.0     1.0   0.0\n",
       "MMRF_1033     64    1.0     1.0   1.0\n",
       "MMRF_1038     69    3.0     1.0   0.0\n",
       "MMRF_1045     81    NaN     0.0   0.0\n",
       "MMRF_1048     53    1.0     0.0   0.0\n",
       "MMRF_1049     68    2.0     0.0   0.0\n",
       "MMRF_1054     58    2.0     0.0   0.0\n",
       "MMRF_1055     66    2.0     1.0   1.0\n",
       "MMRF_1061     70    2.0     0.0   0.0\n",
       "MMRF_1068     58    1.0     0.0   1.0\n",
       "MMRF_1073     62    1.0     0.0   0.0\n",
       "MMRF_1077     67    2.0     1.0   0.0\n",
       "MMRF_1078     45    1.0     1.0   0.0\n",
       "MMRF_1080     66    2.0     1.0   0.0\n",
       "MMRF_1082     47    2.0     1.0   1.0\n",
       "MMRF_1086     66    1.0     0.0   0.0\n",
       "MMRF_1089     61    3.0     1.0   0.0\n",
       "MMRF_1090     68    3.0     0.0   1.0\n",
       "MMRF_1092     69    2.0     1.0   0.0\n",
       "MMRF_1098     49    2.0     1.0   0.0\n",
       "MMRF_1100     68    1.0     0.0   0.0\n",
       "MMRF_1107     82    3.0     1.0   1.0\n",
       "MMRF_1108     85    3.0     0.0   0.0\n",
       "MMRF_1128     48    2.0     0.0   0.0\n",
       "MMRF_1129     81    1.0     1.0   0.0\n",
       "...          ...    ...     ...   ...\n",
       "MMRF_2266     73    2.0     1.0   1.0\n",
       "MMRF_2272     57    3.0     1.0   1.0\n",
       "MMRF_2273     69    NaN     0.0   1.0\n",
       "MMRF_2289     52    1.0     1.0   1.0\n",
       "MMRF_2290     79    2.0     0.0   1.0\n",
       "MMRF_2300     80    3.0     1.0   1.0\n",
       "MMRF_2306     40    2.0     1.0   1.0\n",
       "MMRF_2308     74    1.0     0.0   1.0\n",
       "MMRF_2314     59    2.0     1.0   1.0\n",
       "MMRF_2335     64    3.0     1.0   1.0\n",
       "MMRF_2344     66    1.0     1.0   1.0\n",
       "MMRF_2364     51    3.0     1.0   1.0\n",
       "MMRF_2373     67    1.0     1.0   1.0\n",
       "MMRF_2379     79    3.0     1.0   1.0\n",
       "MMRF_2386     64    2.0     1.0   1.0\n",
       "MMRF_2404     50    3.0     1.0   1.0\n",
       "MMRF_2412     65    3.0     0.0   1.0\n",
       "MMRF_2444     81    3.0     0.0   1.0\n",
       "MMRF_2455     63    2.0     1.0   1.0\n",
       "MMRF_2457     52    3.0     0.0   1.0\n",
       "MMRF_2473     74    1.0     0.0   1.0\n",
       "MMRF_2477     61    3.0     0.0   1.0\n",
       "MMRF_2478     52    NaN     1.0   1.0\n",
       "MMRF_2480     63    NaN     1.0   1.0\n",
       "MMRF_2490     81    3.0     0.0   1.0\n",
       "MMRF_2535     46    3.0     0.0   1.0\n",
       "MMRF_2554     65    1.0     1.0   1.0\n",
       "MMRF_2557     83    3.0     1.0   1.0\n",
       "MMRF_2562     66    2.0     1.0   1.0\n",
       "MMRF_2596     73    1.0     0.0   1.0\n",
       "\n",
       "[482 rows x 4 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Check for null values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "D_Age      0\n",
       "D_ISS     18\n",
       "Gender     0\n",
       "FLAG       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Fill the null values with a mode value\n",
    "\n",
    "Since there are null values in our data we need to fill this up using the most common ISS stage. The mean is not approriate in this case since they are just labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data.D_ISS.fillna(1.0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Read and combine the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "consequence_df = pd.read_csv('../consequence_data_percentage_snvs.csv')\n",
    "#consequence_df2 = pd.read_csv('../consequence_data_percentage_indels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "consequence_df.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "consequence_df.insert(0,'Patient',clinical_df['Patient'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "consequence_df.to_csv('model2/consequence_data_percentage_snvs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#consequence_df['TF_binding_site_variant'] = consequence_df2['TF_binding_site_variant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "missense_variant         737\n",
       "synonymous_variant       737\n",
       "frameshift_variant       737\n",
       "inframe_deletion         737\n",
       "stop_gained              737\n",
       "stop_lost                737\n",
       "stop_retained_variant    737\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consequence_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "consequence_df.TF_binding_site_variant.fillna(consequence_df.TF_binding_site_variant.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Reset the index in order to merge the DFs\n",
    "training_data.reset_index(inplace=True)\n",
    "training_data.drop('Patient', axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "labels ['index'] not contained in axis",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-a384c575e2c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtraining_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsequence_df\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtraining_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/ckibet/lustre/miniconda2/envs/dream_challenge/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, level, inplace, errors)\u001b[0m\n\u001b[1;32m   1875\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1876\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1877\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1878\u001b[0m             \u001b[0mdropped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ckibet/lustre/miniconda2/envs/dream_challenge/lib/python2.7/site-packages/pandas/indexes/base.pyc\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   3049\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3050\u001b[0m                 raise ValueError('labels %s not contained in axis' %\n\u001b[0;32m-> 3051\u001b[0;31m                                  labels[mask])\n\u001b[0m\u001b[1;32m   3052\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3053\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: labels ['index'] not contained in axis"
     ]
    }
   ],
   "source": [
    "training_df = pd.concat([training_data, consequence_df], axis=1)\n",
    "training_df.drop('index', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "labels ['level_0'] not contained in axis",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-afa395da4b44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'level_0'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/ckibet/lustre/miniconda2/envs/dream_challenge/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, level, inplace, errors)\u001b[0m\n\u001b[1;32m   1875\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1876\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1877\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1878\u001b[0m             \u001b[0mdropped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ckibet/lustre/miniconda2/envs/dream_challenge/lib/python2.7/site-packages/pandas/indexes/base.pyc\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   3049\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3050\u001b[0m                 raise ValueError('labels %s not contained in axis' %\n\u001b[0;32m-> 3051\u001b[0;31m                                  labels[mask])\n\u001b[0m\u001b[1;32m   3052\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3053\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: labels ['level_0'] not contained in axis"
     ]
    }
   ],
   "source": [
    "training_df.drop('level_0',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'D_Age', u'D_ISS', u'Gender', u'FLAG', u'missense_variant',\n",
       "       u'synonymous_variant', u'frameshift_variant', u'inframe_deletion',\n",
       "       u'stop_gained', u'stop_lost', u'stop_retained_variant'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'HR_FLAG'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-e5de169c8aba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtraining_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'HR_FLAG'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'FALSE'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'TRUE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtraining_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'FLAG'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'HR_FLAG'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TRUE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtraining_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'HR_FLAG'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ckibet/lustre/miniconda2/envs/dream_challenge/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1995\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1996\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1997\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1999\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ckibet/lustre/miniconda2/envs/dream_challenge/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2002\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2003\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2004\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2006\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ckibet/lustre/miniconda2/envs/dream_challenge/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1348\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ckibet/lustre/miniconda2/envs/dream_challenge/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   3288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3289\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3290\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3291\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3292\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ckibet/lustre/miniconda2/envs/dream_challenge/lib/python2.7/site-packages/pandas/indexes/base.pyc\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   1945\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1946\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1947\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1949\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:4154)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:4018)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12368)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12322)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'HR_FLAG'"
     ]
    }
   ],
   "source": [
    "training_df = training_df.loc[training_df['HR_FLAG'].isin(['FALSE','TRUE'])]\n",
    "\n",
    "training_df['FLAG'] = pd.get_dummies(training_df['HR_FLAG'])['TRUE']\n",
    "training_df.drop('HR_FLAG', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'D_Gender'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-08e061323c3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Gender'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'D_Gender'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Male'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtraining_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'D_Gender'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ckibet/lustre/miniconda2/envs/dream_challenge/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1995\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1996\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1997\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1999\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ckibet/lustre/miniconda2/envs/dream_challenge/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2002\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2003\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2004\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2006\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ckibet/lustre/miniconda2/envs/dream_challenge/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1348\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ckibet/lustre/miniconda2/envs/dream_challenge/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   3288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3289\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3290\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3291\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3292\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ckibet/lustre/miniconda2/envs/dream_challenge/lib/python2.7/site-packages/pandas/indexes/base.pyc\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   1945\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1946\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1947\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1949\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:4154)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:4018)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12368)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12322)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'D_Gender'"
     ]
    }
   ],
   "source": [
    "training_df['Gender'] = pd.get_dummies(training_df['D_Gender'])['Male']\n",
    "training_df.drop('D_Gender', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>D_Age</th>\n",
       "      <th>D_ISS</th>\n",
       "      <th>Gender</th>\n",
       "      <th>FLAG</th>\n",
       "      <th>missense_variant</th>\n",
       "      <th>synonymous_variant</th>\n",
       "      <th>frameshift_variant</th>\n",
       "      <th>inframe_deletion</th>\n",
       "      <th>stop_gained</th>\n",
       "      <th>stop_lost</th>\n",
       "      <th>stop_retained_variant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.034015</td>\n",
       "      <td>25.281544</td>\n",
       "      <td>2.321305</td>\n",
       "      <td>17.398299</td>\n",
       "      <td>2.803953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.074547</td>\n",
       "      <td>21.692411</td>\n",
       "      <td>2.820685</td>\n",
       "      <td>22.095366</td>\n",
       "      <td>2.316991</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.657895</td>\n",
       "      <td>21.107456</td>\n",
       "      <td>3.207237</td>\n",
       "      <td>21.957237</td>\n",
       "      <td>2.850877</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.219298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>65.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.338462</td>\n",
       "      <td>22.430769</td>\n",
       "      <td>3.076923</td>\n",
       "      <td>20.707692</td>\n",
       "      <td>3.415385</td>\n",
       "      <td>0.030769</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.450205</td>\n",
       "      <td>27.796726</td>\n",
       "      <td>0.852660</td>\n",
       "      <td>2.933151</td>\n",
       "      <td>2.967258</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>76.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62.810133</td>\n",
       "      <td>25.594150</td>\n",
       "      <td>2.115435</td>\n",
       "      <td>5.562810</td>\n",
       "      <td>3.891355</td>\n",
       "      <td>0.026116</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>64.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>66.615432</td>\n",
       "      <td>26.375653</td>\n",
       "      <td>1.198893</td>\n",
       "      <td>2.920381</td>\n",
       "      <td>2.643713</td>\n",
       "      <td>0.215186</td>\n",
       "      <td>0.030741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>69.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.684912</td>\n",
       "      <td>25.646013</td>\n",
       "      <td>1.472631</td>\n",
       "      <td>3.639900</td>\n",
       "      <td>3.417616</td>\n",
       "      <td>0.138927</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>81.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.746214</td>\n",
       "      <td>25.045427</td>\n",
       "      <td>0.787402</td>\n",
       "      <td>2.907329</td>\n",
       "      <td>2.119927</td>\n",
       "      <td>0.393701</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>53.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.733519</td>\n",
       "      <td>24.271123</td>\n",
       "      <td>2.525534</td>\n",
       "      <td>20.965645</td>\n",
       "      <td>1.374188</td>\n",
       "      <td>0.129991</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>68.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.850806</td>\n",
       "      <td>23.850806</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>17.197581</td>\n",
       "      <td>2.459677</td>\n",
       "      <td>0.141129</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>58.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.837578</td>\n",
       "      <td>21.672273</td>\n",
       "      <td>2.642960</td>\n",
       "      <td>24.843825</td>\n",
       "      <td>2.835175</td>\n",
       "      <td>0.144161</td>\n",
       "      <td>0.024027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>66.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.499459</td>\n",
       "      <td>27.894395</td>\n",
       "      <td>1.969271</td>\n",
       "      <td>14.196061</td>\n",
       "      <td>3.375893</td>\n",
       "      <td>0.064921</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>70.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.139729</td>\n",
       "      <td>24.337852</td>\n",
       "      <td>2.064651</td>\n",
       "      <td>16.850886</td>\n",
       "      <td>2.377477</td>\n",
       "      <td>0.104275</td>\n",
       "      <td>0.125130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>58.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>49.480757</td>\n",
       "      <td>21.095500</td>\n",
       "      <td>2.423132</td>\n",
       "      <td>24.679291</td>\n",
       "      <td>2.260232</td>\n",
       "      <td>0.061087</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>62.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.870841</td>\n",
       "      <td>26.418787</td>\n",
       "      <td>1.800391</td>\n",
       "      <td>1.545988</td>\n",
       "      <td>4.324853</td>\n",
       "      <td>0.039139</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>67.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.381324</td>\n",
       "      <td>19.492278</td>\n",
       "      <td>3.532753</td>\n",
       "      <td>29.806497</td>\n",
       "      <td>2.733890</td>\n",
       "      <td>0.053258</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>45.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.162493</td>\n",
       "      <td>29.521425</td>\n",
       "      <td>1.530328</td>\n",
       "      <td>1.975515</td>\n",
       "      <td>2.712855</td>\n",
       "      <td>0.027824</td>\n",
       "      <td>0.069560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>66.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>67.454545</td>\n",
       "      <td>25.900826</td>\n",
       "      <td>0.809917</td>\n",
       "      <td>1.966942</td>\n",
       "      <td>3.685950</td>\n",
       "      <td>0.165289</td>\n",
       "      <td>0.016529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>47.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>67.854695</td>\n",
       "      <td>24.994288</td>\n",
       "      <td>1.804889</td>\n",
       "      <td>2.924377</td>\n",
       "      <td>2.398903</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>66.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>67.769608</td>\n",
       "      <td>23.872549</td>\n",
       "      <td>1.274510</td>\n",
       "      <td>2.303922</td>\n",
       "      <td>4.705882</td>\n",
       "      <td>0.073529</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>61.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.147901</td>\n",
       "      <td>26.836832</td>\n",
       "      <td>1.693702</td>\n",
       "      <td>2.695611</td>\n",
       "      <td>3.602099</td>\n",
       "      <td>0.023855</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>68.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>49.105754</td>\n",
       "      <td>23.276309</td>\n",
       "      <td>3.188180</td>\n",
       "      <td>21.695179</td>\n",
       "      <td>2.604977</td>\n",
       "      <td>0.129601</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>69.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.559754</td>\n",
       "      <td>23.396309</td>\n",
       "      <td>2.130931</td>\n",
       "      <td>20.628295</td>\n",
       "      <td>2.218805</td>\n",
       "      <td>0.065905</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.282596</td>\n",
       "      <td>27.046695</td>\n",
       "      <td>1.288660</td>\n",
       "      <td>2.077016</td>\n",
       "      <td>3.183748</td>\n",
       "      <td>0.090964</td>\n",
       "      <td>0.030321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>68.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.133359</td>\n",
       "      <td>23.710355</td>\n",
       "      <td>2.904089</td>\n",
       "      <td>18.551777</td>\n",
       "      <td>1.528468</td>\n",
       "      <td>0.057318</td>\n",
       "      <td>0.114635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>82.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>66.501323</td>\n",
       "      <td>25.562169</td>\n",
       "      <td>1.058201</td>\n",
       "      <td>4.133598</td>\n",
       "      <td>2.612434</td>\n",
       "      <td>0.099206</td>\n",
       "      <td>0.033069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>85.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.648473</td>\n",
       "      <td>24.937406</td>\n",
       "      <td>1.301953</td>\n",
       "      <td>4.031047</td>\n",
       "      <td>4.056084</td>\n",
       "      <td>0.025038</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>48.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.610162</td>\n",
       "      <td>23.452407</td>\n",
       "      <td>3.039716</td>\n",
       "      <td>19.436432</td>\n",
       "      <td>3.372532</td>\n",
       "      <td>0.088751</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>81.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.777347</td>\n",
       "      <td>27.067669</td>\n",
       "      <td>0.800388</td>\n",
       "      <td>3.516857</td>\n",
       "      <td>2.449673</td>\n",
       "      <td>0.169779</td>\n",
       "      <td>0.218288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48.078072</td>\n",
       "      <td>23.999203</td>\n",
       "      <td>2.230631</td>\n",
       "      <td>23.302131</td>\n",
       "      <td>2.330213</td>\n",
       "      <td>0.059749</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>45.926385</td>\n",
       "      <td>24.710505</td>\n",
       "      <td>2.295285</td>\n",
       "      <td>25.289495</td>\n",
       "      <td>1.736973</td>\n",
       "      <td>0.041356</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.941066</td>\n",
       "      <td>20.322732</td>\n",
       "      <td>3.227315</td>\n",
       "      <td>22.988775</td>\n",
       "      <td>1.473340</td>\n",
       "      <td>0.023386</td>\n",
       "      <td>0.023386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>45.755173</td>\n",
       "      <td>22.954941</td>\n",
       "      <td>3.190872</td>\n",
       "      <td>25.043512</td>\n",
       "      <td>2.997486</td>\n",
       "      <td>0.058016</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.788991</td>\n",
       "      <td>20.678899</td>\n",
       "      <td>2.495413</td>\n",
       "      <td>22.513761</td>\n",
       "      <td>1.449541</td>\n",
       "      <td>0.073394</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>45.966761</td>\n",
       "      <td>24.341305</td>\n",
       "      <td>3.060397</td>\n",
       "      <td>24.219700</td>\n",
       "      <td>2.391569</td>\n",
       "      <td>0.020268</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.813279</td>\n",
       "      <td>22.224289</td>\n",
       "      <td>3.180212</td>\n",
       "      <td>20.085550</td>\n",
       "      <td>2.603682</td>\n",
       "      <td>0.055793</td>\n",
       "      <td>0.037195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48.634172</td>\n",
       "      <td>22.460275</td>\n",
       "      <td>2.874487</td>\n",
       "      <td>23.103017</td>\n",
       "      <td>2.731655</td>\n",
       "      <td>0.035708</td>\n",
       "      <td>0.160686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46.855761</td>\n",
       "      <td>23.988566</td>\n",
       "      <td>3.276165</td>\n",
       "      <td>23.262973</td>\n",
       "      <td>2.572559</td>\n",
       "      <td>0.021988</td>\n",
       "      <td>0.021988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54.419138</td>\n",
       "      <td>24.437760</td>\n",
       "      <td>2.177338</td>\n",
       "      <td>17.361410</td>\n",
       "      <td>1.389486</td>\n",
       "      <td>0.214869</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49.019166</td>\n",
       "      <td>22.209696</td>\n",
       "      <td>2.863585</td>\n",
       "      <td>23.427283</td>\n",
       "      <td>2.344983</td>\n",
       "      <td>0.022548</td>\n",
       "      <td>0.112740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.186658</td>\n",
       "      <td>22.025645</td>\n",
       "      <td>3.116377</td>\n",
       "      <td>21.944490</td>\n",
       "      <td>2.645674</td>\n",
       "      <td>0.081156</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.459175</td>\n",
       "      <td>20.804809</td>\n",
       "      <td>2.537986</td>\n",
       "      <td>24.027384</td>\n",
       "      <td>2.153949</td>\n",
       "      <td>0.016697</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.867744</td>\n",
       "      <td>24.456413</td>\n",
       "      <td>1.974865</td>\n",
       "      <td>20.706164</td>\n",
       "      <td>1.915021</td>\n",
       "      <td>0.079793</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49.601276</td>\n",
       "      <td>23.305423</td>\n",
       "      <td>2.671451</td>\n",
       "      <td>21.630781</td>\n",
       "      <td>2.651515</td>\n",
       "      <td>0.039872</td>\n",
       "      <td>0.099681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.827331</td>\n",
       "      <td>21.794009</td>\n",
       "      <td>2.423426</td>\n",
       "      <td>20.918883</td>\n",
       "      <td>1.952205</td>\n",
       "      <td>0.084147</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.273764</td>\n",
       "      <td>19.802281</td>\n",
       "      <td>2.737643</td>\n",
       "      <td>23.011407</td>\n",
       "      <td>2.144487</td>\n",
       "      <td>0.030418</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.801315</td>\n",
       "      <td>23.608473</td>\n",
       "      <td>2.468955</td>\n",
       "      <td>23.564646</td>\n",
       "      <td>2.439737</td>\n",
       "      <td>0.058437</td>\n",
       "      <td>0.058437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.468827</td>\n",
       "      <td>22.174700</td>\n",
       "      <td>2.714262</td>\n",
       "      <td>22.240500</td>\n",
       "      <td>2.319460</td>\n",
       "      <td>0.049350</td>\n",
       "      <td>0.032900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.980355</td>\n",
       "      <td>21.673004</td>\n",
       "      <td>2.867554</td>\n",
       "      <td>20.991762</td>\n",
       "      <td>2.423954</td>\n",
       "      <td>0.031686</td>\n",
       "      <td>0.031686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48.300117</td>\n",
       "      <td>22.723720</td>\n",
       "      <td>3.067605</td>\n",
       "      <td>23.915592</td>\n",
       "      <td>1.856194</td>\n",
       "      <td>0.136772</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46.895973</td>\n",
       "      <td>24.811242</td>\n",
       "      <td>3.439597</td>\n",
       "      <td>23.175336</td>\n",
       "      <td>1.593960</td>\n",
       "      <td>0.062919</td>\n",
       "      <td>0.020973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49.208415</td>\n",
       "      <td>22.793320</td>\n",
       "      <td>2.494036</td>\n",
       "      <td>22.684884</td>\n",
       "      <td>2.689221</td>\n",
       "      <td>0.130124</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49.674338</td>\n",
       "      <td>19.916052</td>\n",
       "      <td>2.605297</td>\n",
       "      <td>24.735852</td>\n",
       "      <td>2.967144</td>\n",
       "      <td>0.086843</td>\n",
       "      <td>0.014474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49.122807</td>\n",
       "      <td>23.269713</td>\n",
       "      <td>2.795450</td>\n",
       "      <td>21.399653</td>\n",
       "      <td>3.393098</td>\n",
       "      <td>0.019279</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.537827</td>\n",
       "      <td>22.660452</td>\n",
       "      <td>2.850484</td>\n",
       "      <td>22.194335</td>\n",
       "      <td>1.649337</td>\n",
       "      <td>0.107565</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.010191</td>\n",
       "      <td>21.956016</td>\n",
       "      <td>2.664044</td>\n",
       "      <td>21.598427</td>\n",
       "      <td>2.735562</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49.578527</td>\n",
       "      <td>22.227152</td>\n",
       "      <td>3.571429</td>\n",
       "      <td>21.739130</td>\n",
       "      <td>2.617569</td>\n",
       "      <td>0.266193</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49.756573</td>\n",
       "      <td>22.317429</td>\n",
       "      <td>2.765336</td>\n",
       "      <td>22.882181</td>\n",
       "      <td>2.200584</td>\n",
       "      <td>0.019474</td>\n",
       "      <td>0.058423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49.959184</td>\n",
       "      <td>21.877551</td>\n",
       "      <td>2.693878</td>\n",
       "      <td>23.551020</td>\n",
       "      <td>1.918367</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>737 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     D_Age  D_ISS  Gender  FLAG  missense_variant  synonymous_variant  \\\n",
       "0     56.0    1.0     1.0   0.0         52.034015           25.281544   \n",
       "1     54.0    1.0     0.0   0.0         51.074547           21.692411   \n",
       "2     46.0    1.0     1.0   0.0         50.657895           21.107456   \n",
       "3     65.0    1.0     0.0   0.0         50.338462           22.430769   \n",
       "4     62.0    1.0     1.0   0.0         65.450205           27.796726   \n",
       "5     76.0    2.0     1.0   0.0         62.810133           25.594150   \n",
       "6     64.0    1.0     1.0   1.0         66.615432           26.375653   \n",
       "7     69.0    3.0     1.0   0.0         65.684912           25.646013   \n",
       "8     81.0    1.0     0.0   0.0         68.746214           25.045427   \n",
       "9     53.0    1.0     0.0   0.0         50.733519           24.271123   \n",
       "10    68.0    2.0     0.0   0.0         53.850806           23.850806   \n",
       "11    58.0    2.0     0.0   0.0         47.837578           21.672273   \n",
       "12    66.0    2.0     1.0   1.0         52.499459           27.894395   \n",
       "13    70.0    2.0     0.0   0.0         54.139729           24.337852   \n",
       "14    58.0    1.0     0.0   1.0         49.480757           21.095500   \n",
       "15    62.0    1.0     0.0   0.0         65.870841           26.418787   \n",
       "16    67.0    2.0     1.0   0.0         44.381324           19.492278   \n",
       "17    45.0    1.0     1.0   0.0         64.162493           29.521425   \n",
       "18    66.0    2.0     1.0   0.0         67.454545           25.900826   \n",
       "19    47.0    2.0     1.0   1.0         67.854695           24.994288   \n",
       "20    66.0    1.0     0.0   0.0         67.769608           23.872549   \n",
       "21    61.0    3.0     1.0   0.0         65.147901           26.836832   \n",
       "22    68.0    3.0     0.0   1.0         49.105754           23.276309   \n",
       "23    69.0    2.0     1.0   0.0         51.559754           23.396309   \n",
       "24    49.0    2.0     1.0   0.0         66.282596           27.046695   \n",
       "25    68.0    1.0     0.0   0.0         53.133359           23.710355   \n",
       "26    82.0    3.0     1.0   1.0         66.501323           25.562169   \n",
       "27    85.0    3.0     0.0   0.0         65.648473           24.937406   \n",
       "28    48.0    2.0     0.0   0.0         50.610162           23.452407   \n",
       "29    81.0    1.0     1.0   0.0         65.777347           27.067669   \n",
       "..     ...    ...     ...   ...               ...                 ...   \n",
       "707    NaN    NaN     NaN   NaN         48.078072           23.999203   \n",
       "708    NaN    NaN     NaN   NaN         45.926385           24.710505   \n",
       "709    NaN    NaN     NaN   NaN         51.941066           20.322732   \n",
       "710    NaN    NaN     NaN   NaN         45.755173           22.954941   \n",
       "711    NaN    NaN     NaN   NaN         52.788991           20.678899   \n",
       "712    NaN    NaN     NaN   NaN         45.966761           24.341305   \n",
       "713    NaN    NaN     NaN   NaN         51.813279           22.224289   \n",
       "714    NaN    NaN     NaN   NaN         48.634172           22.460275   \n",
       "715    NaN    NaN     NaN   NaN         46.855761           23.988566   \n",
       "716    NaN    NaN     NaN   NaN         54.419138           24.437760   \n",
       "717    NaN    NaN     NaN   NaN         49.019166           22.209696   \n",
       "718    NaN    NaN     NaN   NaN         50.186658           22.025645   \n",
       "719    NaN    NaN     NaN   NaN         50.459175           20.804809   \n",
       "720    NaN    NaN     NaN   NaN         50.867744           24.456413   \n",
       "721    NaN    NaN     NaN   NaN         49.601276           23.305423   \n",
       "722    NaN    NaN     NaN   NaN         52.827331           21.794009   \n",
       "723    NaN    NaN     NaN   NaN         52.273764           19.802281   \n",
       "724    NaN    NaN     NaN   NaN         47.801315           23.608473   \n",
       "725    NaN    NaN     NaN   NaN         50.468827           22.174700   \n",
       "726    NaN    NaN     NaN   NaN         51.980355           21.673004   \n",
       "727    NaN    NaN     NaN   NaN         48.300117           22.723720   \n",
       "728    NaN    NaN     NaN   NaN         46.895973           24.811242   \n",
       "729    NaN    NaN     NaN   NaN         49.208415           22.793320   \n",
       "730    NaN    NaN     NaN   NaN         49.674338           19.916052   \n",
       "731    NaN    NaN     NaN   NaN         49.122807           23.269713   \n",
       "732    NaN    NaN     NaN   NaN         50.537827           22.660452   \n",
       "733    NaN    NaN     NaN   NaN         51.010191           21.956016   \n",
       "734    NaN    NaN     NaN   NaN         49.578527           22.227152   \n",
       "735    NaN    NaN     NaN   NaN         49.756573           22.317429   \n",
       "736    NaN    NaN     NaN   NaN         49.959184           21.877551   \n",
       "\n",
       "     frameshift_variant  inframe_deletion  stop_gained  stop_lost  \\\n",
       "0              2.321305         17.398299     2.803953   0.000000   \n",
       "1              2.820685         22.095366     2.316991   0.000000   \n",
       "2              3.207237         21.957237     2.850877   0.000000   \n",
       "3              3.076923         20.707692     3.415385   0.030769   \n",
       "4              0.852660          2.933151     2.967258   0.000000   \n",
       "5              2.115435          5.562810     3.891355   0.026116   \n",
       "6              1.198893          2.920381     2.643713   0.215186   \n",
       "7              1.472631          3.639900     3.417616   0.138927   \n",
       "8              0.787402          2.907329     2.119927   0.393701   \n",
       "9              2.525534         20.965645     1.374188   0.129991   \n",
       "10             2.500000         17.197581     2.459677   0.141129   \n",
       "11             2.642960         24.843825     2.835175   0.144161   \n",
       "12             1.969271         14.196061     3.375893   0.064921   \n",
       "13             2.064651         16.850886     2.377477   0.104275   \n",
       "14             2.423132         24.679291     2.260232   0.061087   \n",
       "15             1.800391          1.545988     4.324853   0.039139   \n",
       "16             3.532753         29.806497     2.733890   0.053258   \n",
       "17             1.530328          1.975515     2.712855   0.027824   \n",
       "18             0.809917          1.966942     3.685950   0.165289   \n",
       "19             1.804889          2.924377     2.398903   0.000000   \n",
       "20             1.274510          2.303922     4.705882   0.073529   \n",
       "21             1.693702          2.695611     3.602099   0.023855   \n",
       "22             3.188180         21.695179     2.604977   0.129601   \n",
       "23             2.130931         20.628295     2.218805   0.065905   \n",
       "24             1.288660          2.077016     3.183748   0.090964   \n",
       "25             2.904089         18.551777     1.528468   0.057318   \n",
       "26             1.058201          4.133598     2.612434   0.099206   \n",
       "27             1.301953          4.031047     4.056084   0.025038   \n",
       "28             3.039716         19.436432     3.372532   0.088751   \n",
       "29             0.800388          3.516857     2.449673   0.169779   \n",
       "..                  ...               ...          ...        ...   \n",
       "707            2.230631         23.302131     2.330213   0.059749   \n",
       "708            2.295285         25.289495     1.736973   0.041356   \n",
       "709            3.227315         22.988775     1.473340   0.023386   \n",
       "710            3.190872         25.043512     2.997486   0.058016   \n",
       "711            2.495413         22.513761     1.449541   0.073394   \n",
       "712            3.060397         24.219700     2.391569   0.020268   \n",
       "713            3.180212         20.085550     2.603682   0.055793   \n",
       "714            2.874487         23.103017     2.731655   0.035708   \n",
       "715            3.276165         23.262973     2.572559   0.021988   \n",
       "716            2.177338         17.361410     1.389486   0.214869   \n",
       "717            2.863585         23.427283     2.344983   0.022548   \n",
       "718            3.116377         21.944490     2.645674   0.081156   \n",
       "719            2.537986         24.027384     2.153949   0.016697   \n",
       "720            1.974865         20.706164     1.915021   0.079793   \n",
       "721            2.671451         21.630781     2.651515   0.039872   \n",
       "722            2.423426         20.918883     1.952205   0.084147   \n",
       "723            2.737643         23.011407     2.144487   0.030418   \n",
       "724            2.468955         23.564646     2.439737   0.058437   \n",
       "725            2.714262         22.240500     2.319460   0.049350   \n",
       "726            2.867554         20.991762     2.423954   0.031686   \n",
       "727            3.067605         23.915592     1.856194   0.136772   \n",
       "728            3.439597         23.175336     1.593960   0.062919   \n",
       "729            2.494036         22.684884     2.689221   0.130124   \n",
       "730            2.605297         24.735852     2.967144   0.086843   \n",
       "731            2.795450         21.399653     3.393098   0.019279   \n",
       "732            2.850484         22.194335     1.649337   0.107565   \n",
       "733            2.664044         21.598427     2.735562   0.000000   \n",
       "734            3.571429         21.739130     2.617569   0.266193   \n",
       "735            2.765336         22.882181     2.200584   0.019474   \n",
       "736            2.693878         23.551020     1.918367   0.000000   \n",
       "\n",
       "     stop_retained_variant  \n",
       "0                 0.160883  \n",
       "1                 0.000000  \n",
       "2                 0.219298  \n",
       "3                 0.000000  \n",
       "4                 0.000000  \n",
       "5                 0.000000  \n",
       "6                 0.030741  \n",
       "7                 0.000000  \n",
       "8                 0.000000  \n",
       "9                 0.000000  \n",
       "10                0.000000  \n",
       "11                0.024027  \n",
       "12                0.000000  \n",
       "13                0.125130  \n",
       "14                0.000000  \n",
       "15                0.000000  \n",
       "16                0.000000  \n",
       "17                0.069560  \n",
       "18                0.016529  \n",
       "19                0.022847  \n",
       "20                0.000000  \n",
       "21                0.000000  \n",
       "22                0.000000  \n",
       "23                0.000000  \n",
       "24                0.030321  \n",
       "25                0.114635  \n",
       "26                0.033069  \n",
       "27                0.000000  \n",
       "28                0.000000  \n",
       "29                0.218288  \n",
       "..                     ...  \n",
       "707               0.000000  \n",
       "708               0.000000  \n",
       "709               0.023386  \n",
       "710               0.000000  \n",
       "711               0.000000  \n",
       "712               0.000000  \n",
       "713               0.037195  \n",
       "714               0.160686  \n",
       "715               0.021988  \n",
       "716               0.000000  \n",
       "717               0.112740  \n",
       "718               0.000000  \n",
       "719               0.000000  \n",
       "720               0.000000  \n",
       "721               0.099681  \n",
       "722               0.000000  \n",
       "723               0.000000  \n",
       "724               0.058437  \n",
       "725               0.032900  \n",
       "726               0.031686  \n",
       "727               0.000000  \n",
       "728               0.020973  \n",
       "729               0.000000  \n",
       "730               0.014474  \n",
       "731               0.000000  \n",
       "732               0.000000  \n",
       "733               0.035759  \n",
       "734               0.000000  \n",
       "735               0.058423  \n",
       "736               0.000000  \n",
       "\n",
       "[737 rows x 11 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Check for multicolinierity\n",
    "\n",
    "From the clustermap below,we can see that there is little multicolinierity except for inframe_deletion and frameshift variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.matrix.ClusterGrid at 0x2aaae7ed1e50>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk4AAAKwCAYAAABuwfCcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2aaae7b4b9d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.clustermap(training_df.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "#from ggplot import *\n",
    "import time\n",
    "import re\n",
    "import seaborn as sns\n",
    "#plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "# packages for checking assumptions\n",
    "from scipy import stats as stats\n",
    "\n",
    "# packages for linear regression and validation \n",
    "import statsmodels.formula.api as sm\n",
    "from sklearn import linear_model\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# the command below means that the output of multiple commands in a cell will be output at once.\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# the command below tells jupyter to display up to 80 columns, this keeps everything visible\n",
    "pd.set_option('display.max_columns', 80)\n",
    "pd.set_option('expand_frame_repr', True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_scores(regressor, X_train, y_train, X_test, y_test, oob_score=True):\n",
    "    print(\"Train score: \" + str(regressor.score(X_train, y_train)))\n",
    "    print(\"Test score: \" + str(regressor.score(X_test, y_test)))\n",
    "    if oob_score:\n",
    "        print(\"OOB score: \" + str(regressor.oob_score_))\n",
    "        \n",
    "def train_linear_r(X_train, y_train, X_test, y_test):\n",
    "    # Create linear regression object\n",
    "    regr = linear_model.LinearRegression()\n",
    "    \n",
    "    # Train the model using the training sets\n",
    "    model1 = regr.fit(X_train, y_train)\n",
    "    \n",
    "    print_scores(model1, X_train, y_train, X_test, y_test, oob_score=False)\n",
    "    plot_test(model1, X_train, y_train, 'train')\n",
    "    plot_test(model1, X_test, y_test, 'test')\n",
    "    \n",
    "def plot_test(regressor, X_test, y_test, data):\n",
    "    rf_y_predicted_test = regressor.predict(X_test)\n",
    "    print(\"Mean squared error: %d \" % round(mean_squared_error(y_test, rf_y_predicted_test)))\n",
    "    fig = plt.figure(figsize=(7, 7))\n",
    "    plt.scatter(y_test, rf_y_predicted_test, alpha=0.5)\n",
    "    plt.xlabel('True value')\n",
    "    plt.ylabel('Predicted value')\n",
    "    plt.title(\"Relationship between True Y and predicted Y* (%s dataset)\" % data)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Prapare the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_train_data(train_df, pred_var):\n",
    "    \"\"\"\n",
    "    Given a dataframe and the predictive variable, this\n",
    "    function returns train and test arrays generated using  the  \n",
    "    scikit--learn split function\n",
    "    \"\"\"\n",
    "    rf_trainRes = train_df[pred_var]\n",
    "    train_df = train_df.drop(pred_var, axis=1)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(train_df, rf_trainRes, test_size=0.2, random_state=42)\n",
    "    \n",
    "    train_df[pred_var] = rf_trainRes\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = prepare_train_data(training_df, 'FLAG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_df.D_PFS_FLAG_18.sum()/train_df.D_PFS_FLAG_18.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decision_tree_c(X_train, y_train, X_test, y_test):\n",
    "    from sklearn.tree import DecisionTr\n",
    "    # Step 1: Initiating the DecisionTreeRegressor algorithm\n",
    "    decision_regressor = DecisionTreeRegressor(random_state=0)\n",
    "    # Step 2: Training the algorithm using the X_train dataset of features and y_train, the associated target features\n",
    "    decision_regressor.fit(X_train, y_train)\n",
    "    # step 3: Print prediction scores\n",
    "    print_scores(decision_regressor, X_train, y_train, X_test, y_test, oob_score=False)\n",
    "    plot_test(decision_regressor, X_train, y_train, 'train')\n",
    "    plot_test(decision_regressor, X_test, y_test, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity check for nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "[16:49:15] src/objective/regression_obj.cc:108: label must be in [0,1] for logistic regression",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-d3275c1c3e38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m               \u001b[0;34m'objective'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'binary:logistic'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'max_depth'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'min_child_weight'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m               'n_estimators':5000, 'reg_alpha':0.005} \n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mcomplete\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_xgb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mour_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mtest_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfeat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeat_columns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-5642d9cbc4e4>\u001b[0m in \u001b[0;36mtrain_xgb\u001b[0;34m(feature_frame, feature_frame_p, y_train, y_test, our_params)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mxgdmat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mmy_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mour_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxgdmat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0mtestdmat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_frame_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestdmat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ckibet/lustre/miniconda2/envs/dream_challenge/lib/python2.7/site-packages/xgboost/training.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, learning_rates, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    203\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ckibet/lustre/miniconda2/envs/dream_challenge/lib/python2.7/site-packages/xgboost/training.pyc\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ckibet/lustre/miniconda2/envs/dream_challenge/lib/python2.7/site-packages/xgboost/core.pyc\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m             \u001b[0m_check_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBoosterUpdateOneIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ckibet/lustre/miniconda2/envs/dream_challenge/lib/python2.7/site-packages/xgboost/core.pyc\u001b[0m in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \"\"\"\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mXGBoostError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mXGBoostError\u001b[0m: [16:49:15] src/objective/regression_obj.cc:108: label must be in [0,1] for logistic regression"
     ]
    }
   ],
   "source": [
    "feat_columns = training_df.columns\n",
    "feat_columns = list(feat_columns)\n",
    "X_train, X_test, y_train, y_test = prepare_train_data(training_df, 'FLAG')\n",
    "print \"Complete\"\n",
    "our_params = {'eta': 0.3, 'seed':0, 'subsample': 1, 'colsample_bytree': 1,'scale_pos_weight': 1, \n",
    "              'objective': 'binary:logistic', 'max_depth':10, 'min_child_weight':1,\n",
    "              'n_estimators':5000, 'reg_alpha':0.005} \n",
    "complete = train_xgb(X_train,X_test,y_train, y_test,our_params)\n",
    "test_list = []\n",
    "for feat in feat_columns:\n",
    "    poped = feat_columns.pop()\n",
    "    if poped == 'FLAG':\n",
    "        feat_columns.insert(0,poped)\n",
    "        continue\n",
    "    else:\n",
    "        print poped\n",
    "        training_df_2 = training_df[feat_columns]\n",
    "        X_train, X_test, y_train, y_test = prepare_train_data(training_df_2, 'FLAG')\n",
    "        test = train_xgb(X_train,X_test,y_train, y_test,our_params)\n",
    "        test_list.append((poped,test))\n",
    "        feat_columns.insert(0,poped)\n",
    "        \n",
    "feat_importance = pd.DataFrame(test_list)\n",
    "feat_importance['importance'] = complete - feat_importance[1]\n",
    "feat_importance.columns = [['feature', 'auPRC','importance']]\n",
    "feat_importance.sort_values(by='importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52525987206691049"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "#Accuracy metrics\n",
    "from sklearn.metrics import accuracy_score, classification_report, auc\n",
    "from sklearn.metrics import accuracy_score, classification_report, auc\n",
    "from sklearn.metrics import precision_recall_curve, roc_auc_score\n",
    "\n",
    "from sklearn import feature_selection\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "def train_sgd(feature_frame, feature_frame_p, y_train, y_test):\n",
    "    scaler = MinMaxScaler()\n",
    "    #Scale the train data\n",
    "    scaler.fit(feature_frame) \n",
    "    X_train = scaler.transform(feature_frame)\n",
    "\n",
    "    #Scale the test data as well\n",
    "    scaler.fit(feature_frame_p)\n",
    "    X_test = scaler.transform(feature_frame_p)\n",
    "    \n",
    "    clf = SGDClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "    pred_sgd = clf.predict(X_test)\n",
    "    \n",
    "    return roc_auc_score(y_test, pred_sgd)\n",
    "\n",
    "def train_svm(feature_frame, feature_frame_p, y_train, y_test):\n",
    "    scaler = MinMaxScaler()\n",
    "    #Scale the train data\n",
    "    scaler.fit(feature_frame) \n",
    "    X_train = scaler.transform(feature_frame)\n",
    "\n",
    "    #Scale the test data as well\n",
    "    scaler.fit(feature_frame_p)\n",
    "    X_test = scaler.transform(feature_frame_p)\n",
    "    \n",
    "    clf = svm.SVC()\n",
    "    clf.fit(X_train, y_train)\n",
    "    pred_svm = clf.predict(X_test)\n",
    "    \n",
    "    print(\"auPRC score: %f\" % scikitlearn_calc_auPRC(y_test, pred_svm))\n",
    "    \n",
    "    return roc_auc_score(y_test, pred_svm)\n",
    "\n",
    "def train_xgb(feature_frame,feature_frame_p,y_train, y_test, our_params):\n",
    "    \n",
    "    xgdmat = xgb.DMatrix(feature_frame, y_train)\n",
    "    my_model = xgb.train(our_params,xgdmat)\n",
    "    testdmat = xgb.DMatrix(feature_frame_p, y_test)\n",
    "    y_pred = my_model.predict(testdmat)\n",
    "    \n",
    "    plot_feature_importance(my_model, 'Features.png')\n",
    "    \n",
    "    print(\"AUC score: %f\" % roc_auc_score(y_test, y_pred))\n",
    "    \n",
    "    return scikitlearn_calc_auPRC(y_test, y_pred)\n",
    "\n",
    "def train_gradient(feature_frame,feature_frame_p,y_train, y_test):\n",
    "    clf = GradientBoostingClassifier()\n",
    "    clf.fit(feature_frame, y_train)\n",
    "    pred_sgd = clf.predict(feature_frame_p)\n",
    "    \n",
    "    print(\"auPRC score: %f\" % scikitlearn_calc_auPRC(y_test, pred_sgd))\n",
    "\n",
    "    return roc_auc_score(y_test, pred_sgd)\n",
    "\n",
    "def train_randomF(feature_frame,feature_frame_p,y_train, y_test):\n",
    "    rf_classifier = RandomForestClassifier(n_estimators=100, oob_score=True)\n",
    "    # Step 2: Training the random regressor algorithm\n",
    "\n",
    "    rf_classifier.fit(feature_frame, y_train)\n",
    "    # Step 3: Calculating the score of the predictive power on the training and testing dataset.\n",
    "    #print_scores(rf_classifier, X_train, y_train, X_test, y_test, oob_score=True)\n",
    "    #clf = GradientBoostingClassifier()\n",
    "    #clf.fit(feature_frame, y_train)\n",
    "    pred_rand = rf_classifier.predict(feature_frame_p)\n",
    "    \n",
    "    print(\"auPRC score: %f\" % scikitlearn_calc_auPRC(y_test, pred_rand))\n",
    "\n",
    "    return roc_auc_score(y_test, pred_rand)\n",
    "\n",
    "def get_auc(y_test, y_pred, lab=1):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred, pos_label=lab)\n",
    "    return metrics.auc(fpr, tpr)\n",
    "\n",
    "def scikitlearn_calc_auPRC(y_true, y_score):\n",
    "    \n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_score)\n",
    "    return auc(recall, precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_feature_importance(xgb_model, fig_out):\n",
    "    sns.set(font_scale = 1.5)\n",
    "    fig, ax = plt.subplots( nrows=1, ncols=1 )  # create figure & 1 axis\n",
    "    xgb.plot_importance(xgb_model, ax=ax)\n",
    "    #ax.plot([0,1,2], [10,20,3])\n",
    "    fig.savefig(fig_out, bbox_inches='tight')   # save the figure to file\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auPRC score: 0.380801\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.53886693017127796"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_gradient(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auPRC score: 0.408579\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.54321475625823445"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_randomF(X_train,X_test,y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auPRC score: 0.506375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.72173913043478266"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_params = {'eta': 0.3, 'seed':0, 'subsample': 1, 'colsample_bytree': 1,'scale_pos_weight': 1, \n",
    "              'objective': 'binary:logistic', 'max_depth':10, 'min_child_weight':1,\n",
    "              'n_estimators':5000, 'reg_alpha':0.005} \n",
    "train_xgb(X_train,X_test,y_train, y_test,our_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auPRC score: 0.611486\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_svm(X_train,X_test,y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verdict\n",
    "\n",
    "From these analysis, we can have very low performance of our models, especially when we compare the AUC and auPRC scores. These scores show that we need to put more focus on feature enginering and parameter optimization. Therefore, thenext stage in this analysis is to optimize the praneters and see how well how models are performing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cyto = 'CYTO_predicted_feature_01\tCYTO_predicted_feature_02\tCYTO_predicted_feature_03\tCYTO_predicted_feature_05\tCYTO_predicted_feature_06\tCYTO_predicted_feature_08\tCYTO_predicted_feature_12\tCYTO_predicted_feature_13\tCYTO_predicted_feature_14\tCYTO_predicted_feature_15\tCYTO_predicted_feature_16\tCYTO_predicted_feature_17\tCYTO_predicted_feature_18'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
